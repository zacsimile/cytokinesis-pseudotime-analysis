{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skimage.measure as measure\n",
    "\n",
    "import line_utils\n",
    "import image_utils\n",
    "import file_utils\n",
    "import pca_utils\n",
    "\n",
    "logger = logging.getLogger('pseudotime')\n",
    "logging.basicConfig(\n",
    "    filename='pseudotime_run.log',\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.DEBUG,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "# logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = file_utils.load_targets('targets.yaml')\n",
    "\n",
    "# IN THE CASE OF USING THE AGGEGRATE TABLE LOADER (CellPose PCA), USE THIS INSTEAD OF TARGETS\n",
    "# file_path = r\"C:\\Users\\AG Ewers\\Desktop\\Cytokinesis PCA Software\\Final_Feature_Excel_File.xlsx\"\n",
    "\n",
    "# Stage to key on\n",
    "stage_key = \"Stage\"\n",
    "\n",
    "# Order of time stages\n",
    "time_key = \"Frame\"\n",
    "\n",
    "# binning\n",
    "n_time_bins = 35         # How many bins do we want?\n",
    "binning = 'equal-size' # 'equal-width' (split PCA fit into equal chunks along line) \n",
    "                        # or 'equal-size' (split fit so the same number of points \n",
    "                        # are in each bin)\n",
    "overlap = 1          # Fraction of the bin size to overlap\n",
    "\n",
    "# Channels per image (TODO: Auto detect)\n",
    "n_ch = 4\n",
    "\n",
    "# wavelengths to be found in the file names\n",
    "# Sublists are grouped. First element of the sublist is a group name.\n",
    "# NOTE: First element must be a number!\n",
    "wvls = [488,[568, 565, \"orange\"],[646,647,657]]\n",
    "\n",
    "length = 500\n",
    "\n",
    "# pixel sizes (we assume they are constant)\n",
    "dx, dy, dz = 0.09, 0.09, 1\n",
    "\n",
    "# What features should we perform PCA on?\n",
    "\n",
    "features = [\n",
    "    \"areashape_maximumradius\", \"areashape_meanradius\", \"areashape_equivalentdiameter\",\n",
    "    \"areashape_minoraxislength\", \"areashape_eccentricity\", \"septin_delta_ef\",\n",
    "    \"diama_micron\", \"diamm_micron\",  \"ratio_diam\", \"abs_key\",\n",
    "    \"intensity_integratedintensity_input_microtubule\", \"intensity_integratedintensity_input_septin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Go to each target's workbook and compute necessary additional metrics,\n",
    "# including distance between septin rings, septin ring diameter, and\n",
    "# microtubule bundle width.\n",
    "for k, v in targets.items():\n",
    "    try:\n",
    "        logger.debug(f\"Accessing {os.path.basename(v['workbook'])}\")\n",
    "    except KeyError:\n",
    "        # Not a target with a workbook\n",
    "        continue\n",
    "\n",
    "    # Pre-cleaned metrics\n",
    "    metrics = file_utils.load_workbooks({k: v})\n",
    "\n",
    "    # Establish columns for septin peak locations (x_septin_1, x_septin_2) and distance between them (dx_septin)\n",
    "    metrics['dx_septin'], metrics['x_septin_1'], metrics['x_septin_2'] = np.nan, np.nan, np.nan\n",
    "    metrics['diamA_micron'] = 0\n",
    "\n",
    "    # Establish empty columns for new metrics\n",
    "    metrics['diamM_micron'] = 0\n",
    "    metrics['delta_diam'] = 0\n",
    "    metrics['ratio_diam'] = 0\n",
    "   \n",
    "    for i, ml in metrics.iterrows():\n",
    "        logger.debug(f\"  Septin ring fit for {os.path.basename(ml['filename'])}\")\n",
    "\n",
    "        # Get the image associated with this row and load it with the channels sorted from high to low\n",
    "        im = image_utils.NDImage(ml[\"filename\"], load_sorted=True)\n",
    "\n",
    "        # get x, y, angle for this row\n",
    "        x, y, angle = ml[[\"X\", \"Y\", \"Angle\"]]\n",
    "\n",
    "        # find wavelengths in file name and sort from high to low\n",
    "        wvls_dict, binned_wvls = image_utils.extract_channel_targets_from_filename(ml[\"filename\"], wvls=wvls)\n",
    "\n",
    "        # Establish target names in this data set and sort from high to low to match image load\n",
    "        channel_targets = [wvls_dict[str(wvl)] for wvl in sorted(binned_wvls)[::-1]]\n",
    "\n",
    "        # the last channel is always DAPI, if unknown\n",
    "        if len(channel_targets) < n_ch:\n",
    "            channel_targets.append(\"DAPI\") \n",
    "\n",
    "        # --------- Fit septin peaks ---------\n",
    "\n",
    "        # ... get the septin peaks\n",
    "        mt_ch = [i for i, t in enumerate(channel_targets) if any([t == n for n in image_utils.target_names(targets, \"MTs\")])][0]\n",
    "        septin_ch = [i for i, t in enumerate(channel_targets) if any([t == n for n in image_utils.target_names(targets, \"septin\")])][0]\n",
    "        im_proj = im[:].mean(1).squeeze()\n",
    "        p0, p1, dX2 = line_utils.find_septin_peaks(im_proj, x, y, angle, length,\n",
    "                                                    mt_ch=mt_ch, \n",
    "                                                    septin_ch=septin_ch)\n",
    "\n",
    "        metrics.loc[i,['x_septin_1','x_septin_2','dx_septin']] = [p0, p1, dX2]\n",
    "\n",
    "        # --------- Get the number of occupied pixels in the MT profile ---------\n",
    "        # This is important for distinguishing abscission from others\n",
    "        xl, xu, yl, yu = line_utils.get_line_profile_endpoints(x, y, angle, length)\n",
    "\n",
    "        chs = measure.profile_line(im_proj, [xl, yu], [xu, yl], linewidth=25)\n",
    "\n",
    "        mt, septin = chs[:,mt_ch], chs[:,septin_ch]\n",
    "\n",
    "        mt_min, mt_max = np.min(mt), np.max(mt)\n",
    "        mt_norm = (mt-mt_min)/(mt_max-mt_min)\n",
    "        metrics.loc[i,'fill_microtubule'] = np.sum(mt_norm)/(length*25)\n",
    "\n",
    "        # --------- Get the ratio of one side of the MT profile to the other ---------\n",
    "        mt_mid = len(mt) // 2\n",
    "        half1, half2 = mt_norm[:mt_mid], mt_norm[mt_mid:][::-1]\n",
    "\n",
    "        # Demand the split be the same length (it should be, but in case it's odd line length)\n",
    "        if len(half1) != len(half2):\n",
    "            clipto = min(len(half1), len(half2))\n",
    "            half1 = half1[:clipto]\n",
    "            half2 = half2[:clipto]\n",
    "\n",
    "        half1s, half2s = half1.sum(), half2.sum()\n",
    "        metrics.loc[i,'balance_microtubule'] = min(half1s,half2s)/max(half1s,half2s)\n",
    "\n",
    "        # Always put the half with more signal in the denominator \n",
    "        if half1s <= half2s:\n",
    "            metrics.loc[i,'balance_microtubule2'] = np.nanmean(half1/(half2+1e-6))\n",
    "            metrics.loc[i,'balance_microtubule3'] = np.nanmean(half2/(half1+1e-6))\n",
    "        else:\n",
    "            metrics.loc[i,'balance_microtubule2'] = np.nanmean(half2/(half1+1e-6))\n",
    "            metrics.loc[i,'balance_microtubule3'] = np.nanmean(half1/(half2+1e-6))\n",
    "\n",
    "        # --------- Get the ratio of septin signal/mt signal ---------\n",
    "        sm_min = min(np.min(septin), np.min(mt))\n",
    "        sm_max = max(np.max(septin), np.max(mt))\n",
    "        septin_norm = (septin-sm_min)/(sm_max-sm_min)\n",
    "        septin_norm2 = (septin-mt_min)/(mt_max-mt_min)\n",
    "        mt_norm2 = (mt-sm_min)/(sm_max-sm_min)\n",
    "        metrics.loc[i,'balance_septin'] = np.mean(septin_norm/(mt_norm2+1e-6))\n",
    "        metrics.loc[i,'balance_septin2'] = np.mean(septin_norm**2/(mt_norm2+1e-6))\n",
    "        metrics.loc[i,'balance_septin3'] = np.mean(septin_norm2/(mt_norm+1e-6))\n",
    "        metrics.loc[i,'balance_septin4'] = np.mean(septin_norm2**2/(mt_norm+1e-6))\n",
    "\n",
    "        # --------- Fit septin rings ---------\n",
    "        p0x, p0y = line_utils.get_image_coordinate_from_distance_along_line(p0, xl, xu, yl, yu, len(septin))\n",
    "        p1x, p1y = line_utils.get_image_coordinate_from_distance_along_line(p1, xl, xu, yl, yu, len(septin))\n",
    "\n",
    "        # Now get the orthogonal line profile at the line center\n",
    "        xl3, xu3, yl3, yu3 = line_utils.get_line_profile_endpoints(p0x, p0y, angle-90, length)\n",
    "        chs = measure.profile_line(im_proj.T, [xl3, yu3], [xu3, yl3], linewidth=25)\n",
    "        septin_ring0 = chs[:,septin_ch]\n",
    "\n",
    "        ring0_diameter, res_lsq_ring0 = line_utils.fit_gaussian_fwhm(septin_ring0, return_dict=True)\n",
    "\n",
    "        # Now get the orthogonal line profile at the line center\n",
    "        xl4, xu4, yl4, yu4 = line_utils.get_line_profile_endpoints(p1x, p1y, angle-90, length)\n",
    "        chs = measure.profile_line(im_proj.T, [xl4, yu4], [xu4, yl4], linewidth=25)\n",
    "        septin_ring1 = chs[:,septin_ch]\n",
    "\n",
    "        ring1_diameter, _ = line_utils.fit_gaussian_fwhm(septin_ring1, return_dict=True)\n",
    "\n",
    "        metrics.loc[i,'diam_septin_ring'] = 0.5*(ring0_diameter + ring1_diameter)\n",
    "\n",
    "        # --------- Fit center MT cross-section ---------\n",
    "        # Now get the orthogonal line profile at the line center\n",
    "        xl2, xu2, yl2, yu2 = line_utils.get_line_profile_endpoints(x, y, angle-90, length)\n",
    "        chs = measure.profile_line(im_proj.T, [xl2, yu2], [xu2, yl2], linewidth=25)\n",
    "        mt = chs[:,mt_ch]\n",
    "\n",
    "        # Zach original\n",
    "        # outer_diameter, res_lsq = line_utils.fit_tubule_diameter(mt, return_dict=True)\n",
    "        # metrics.loc[i,'diam_microtubule'] = outer_diameter\n",
    "\n",
    "        # outer diameter modified for Expansion Factor 24.04.2025\n",
    "        try:\n",
    "            # outer_diameter, res_lsq = line_utils.fit_tubule_diameter(mt, return_dict=True)\n",
    "            outer_diameter, res_lsq = line_utils.fit_gaussian_fwhm(mt, return_dict=True)\n",
    "        except ValueError:\n",
    "            outer_diameter = 0\n",
    "        metrics.loc[i,'diam_microtubule'] = (outer_diameter) * (dx/metrics.loc[i,'EF'])\n",
    "\n",
    "        # --------- prepared from manually measured data ---------\n",
    "        \n",
    "        # Get diamA_micron\n",
    "        metrics.loc[i,'diamA_micron'] = (metrics.loc[i,'FWHM'] / 1000) * (dx/metrics.loc[i,'EF'])\n",
    "        \n",
    "        # Get diamM_micron\n",
    "        metrics.loc[i,'diamM_micron'] = (metrics.loc[i,'diamM'] / 1000) * (dx/metrics.loc[i,'EF'])\n",
    "\n",
    "        # Get delta_diam\n",
    "        metrics.loc[i,'delta_diam'] = ((metrics.loc[i,'diamM']) - (metrics.loc[i,'FWHM'])) / 1000 * (dx/metrics.loc[i,'EF'])\n",
    "\n",
    "        # Get ratio_diam\n",
    "        metrics.loc[i,'ratio_diam'] = (metrics.loc[i,'FWHM']) / (metrics.loc[i,'diamM'])\n",
    "\n",
    "      \n",
    "\n",
    "    # TODO: Should the behaviour be replace or new?\n",
    "    with pd.ExcelWriter(v['workbook'], mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "        metrics.to_excel(writer, sheet_name=f\"{v['workbook_sheet_name']}_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with pd.ExcelWriter(v['workbook'], mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "        metrics.to_excel(writer, sheet_name=f\"{v['workbook_sheet_name']}_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative loader for aggregate metrics\n",
    "# WARNING: IF YOU RUN THIS, DO NOT RUN THE CELL BELOW.\n",
    "metrics = pd.read_excel(file_path)\n",
    "metrics = metrics.dropna(subset=features + [\"stage_key\"])\n",
    "stage_key = \"stage\"  # lowercase in this style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's go grab the new calculations\n",
    "targets_processed = targets\n",
    "for k, v in targets.items():\n",
    "    try:\n",
    "        targets_processed[k][\"workbook_sheet_name\"] = f\"{v['workbook_sheet_name']}_processed\"\n",
    "        targets_processed[k][\"workbook_header_row\"] = 0\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# Load aggregated data from workbooks\n",
    "metrics = file_utils.load_workbooks(targets_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA on the aggregated data and fit a polynomial through the space\n",
    "metrics_features = metrics[features]\n",
    "# metrics_features = metrics[metrics[stage_key] != 'A'][features]\n",
    "pca, coords, fit = pca_utils.pca(metrics_features, transform=True, fit=True)\n",
    "\n",
    "# bins is an index into coords, which 0-indexes whatever is fed as features to pca_utils.pca\n",
    "bins_inds = metrics_features.index\n",
    "\n",
    "# Now add the PCA results as columns\n",
    "xx, yy = coords\n",
    "metrics.loc[bins_inds,\"pca0\"] = xx\n",
    "metrics.loc[bins_inds,\"pca1\"] = yy\n",
    "\n",
    "bins = pca_utils.sort_by_point_plane_dist(xx, yy, fit, nbins=n_time_bins, binning=binning, overlap=overlap)\n",
    "\n",
    "for i, bin in enumerate(bins):\n",
    "    # Deal with the first instance\n",
    "    try:\n",
    "        curr_bin = metrics.loc[bins_inds[bins[i]],time_key]\n",
    "    except KeyError:\n",
    "        metrics.loc[bins_inds[bins[i]],time_key] = int(i)\n",
    "        continue\n",
    "\n",
    "    # Anything that's empty gets set to the current value\n",
    "    metrics.loc[bins_inds[bins[i]][curr_bin.isna()],time_key] = int(i)\n",
    "    # At least one of these is already in one bin, so lets make a duplicate\n",
    "    for j in bin[~curr_bin.isna()]:\n",
    "        metrics.loc[len(metrics)] = metrics.loc[bins_inds[j]]\n",
    "        # metrics.index = metrics.index + 1\n",
    "        # metrics = metrics.sort_index()\n",
    "        metrics.loc[len(metrics)-1,time_key] = int(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALERT! IF YOU DECLINED TO FIT ONE OF THE STAGES\n",
    "# e.g. you set metrics_features = metrics[metrics[stage_key] != 'A'][features]\n",
    "# You must run this cell so they are assigned to the final time point\n",
    "metrics.loc[metrics[metrics[time_key].isna()].index, time_key] = metrics[time_key].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the PCA results\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "stage_keys = {v:k for k,v in enumerate(metrics[stage_key].unique())}\n",
    "# stage_colors = [stage_keys[x] for x in metrics[\"Stage\"]]\n",
    "\n",
    "colors = {'A':'#DDCC77', 'BA':'#999933', 'CS':'#88CCEE', 'RC':'#332288',  'RS':'#44AA99', 'SM':'#117733' }\n",
    "\n",
    "\n",
    "colors_ls = [colors[x] for x in metrics.loc[bins_inds, stage_key]]\n",
    "\n",
    "# colors_ls = [list(colors.values())[x % len(colors)] for x in metrics.loc[bins_inds, time_key].astype(int)]\n",
    "\n",
    "scatter = ax.scatter(xx, yy, c=colors_ls, s=5, alpha=0.7)\n",
    "# scatter = ax.scatter(xx, yy, c=stage_colors, cmap='gist_rainbow_r', s=20)\n",
    "\n",
    "xxx = np.linspace(np.min(xx),np.max(xx),300)\n",
    "ax.plot(xxx, np.poly1d(fit)(xxx), linestyle='--', c='k')\n",
    "\n",
    "dist, xp, yp = pca_utils.point_poly_dist(xx, yy, fit)\n",
    "permutation = np.argsort(xp)\n",
    "\n",
    "# for i in range(len(xp)):\n",
    "#     ax.plot([xx[i], xp[i]], [yy[i], yp[i]], c='b', linewidth=0.5)\n",
    "\n",
    "# ax.legend(scatter.legend_elements(num=len(stage_keys)-1)[0],list(stage_keys.keys()))\n",
    "# ax.legend([patches.Circle((0,0), 1, fc=colors[x]) for x in metrics[stage_key]], list(stage_keys.keys()))\n",
    "ax.set_xlabel('PC0')\n",
    "ax.set_ylabel('PC1')\n",
    "# ax.set_xlim([-4,7])\n",
    "# ax.set_ylim([-2,14])\n",
    "\n",
    "#fig.savefig('20250508_pca_color_s5_opacity7_EF_adjusted_diamM_new_2.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the features ranked from most important (left) to least important (right)\n",
    "# per principle component (row number)\n",
    "pd.DataFrame({f\"feature{i} (explained variance {pca.explained_variance_ratio_[i]:.2f})\": k for i, k in enumerate(np.array(features)[np.argsort(np.abs(pca.components_),axis=1)[:,::-1]])}|\n",
    "             {f\"feature{i}_importance\": k for i, k in enumerate(np.sort(np.abs(pca.components_),axis=1)[:,::-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative save for aggregate metrics\n",
    "# WARNING: IF YOU RUN THIS, DO NOT RUN THE CELL BELOW.\n",
    "\n",
    "# Drop unused columns\n",
    "metrics_merged = metrics.dropna(how='all', axis=1)\n",
    "\n",
    "# And write to file\n",
    "with pd.ExcelWriter(file_path, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "    metrics_merged.to_excel(writer, sheet_name=\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now map the features back onto their original files\n",
    "for k, v in targets_processed.items():\n",
    "    try:\n",
    "        logger.debug(f\"Accessing {os.path.basename(v['workbook'])}\")\n",
    "    except KeyError:\n",
    "        # Not a target with a workbook\n",
    "        continue\n",
    "\n",
    "    # Pre-cleaned metrics\n",
    "    metrics_processed = file_utils.load_workbooks({k: v})\n",
    "\n",
    "    # Now merge in the PCA information\n",
    "    metrics_merged = pd.merge(metrics_processed, metrics, how=\"left\")\n",
    "\n",
    "    # Drop unused columns\n",
    "    metrics_merged = metrics_merged.dropna(how='all', axis=1)\n",
    "\n",
    "    # And write to file\n",
    "    with pd.ExcelWriter(v['workbook'], mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "        metrics_merged.to_excel(writer, sheet_name=v['workbook_sheet_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expansion",
   "language": "python",
   "name": "expansion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
